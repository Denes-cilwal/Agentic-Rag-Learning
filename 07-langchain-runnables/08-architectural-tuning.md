
# LangChain Architectural Tuning: From Chains to Runnables

---

## 1ï¸âƒ£ The problem LangChain created for itself

Early LangChain tried to standardize everything by creating many components:

LLMChain

RetrievalQA

ConversationalRetrievalChain

RouterChain

TransformChain

SequentialChain

MultiPromptChain

â€¦and many more

What went wrong?

Too many classes

Too many abstractions

# LangChain Architectural Tuning: From Chains to Runnables

---

## 1ï¸âƒ£ The Problem LangChain Created for Itself

Early LangChain tried to standardize everything by creating many components:

* `LLMChain`
* `RetrievalQA`
* `ConversationalRetrievalChain`
* `RouterChain`
* `TransformChain`
* `SequentialChain`
* `MultiPromptChain`
* ...and many more

**What went wrong?**

- Too many classes
- Too many abstractions
- Hard to customize
- Hard to compose
- Hard to debug
- Users had to learn LangChain, not just build AI apps

So LangChain recreated the same problem it was trying to solve:

> "Too many chains"

---

## 2ï¸âƒ£ The Insight: Everything is the Same Shape

LangChain authors realized something very important:

Every AI step is just: `input â†’ output`

**Examples:**

- `PromptTemplate`: `dict â†’ string`
- `Retriever`: `string â†’ documents`
- `LLM`: `string â†’ string`
- `OutputParser`: `string â†’ structured data`
- `Tool`: `input â†’ output`

So instead of many types of chains, they needed one universal abstraction.

---

## 3ï¸âƒ£ Enter: Runnables ğŸ§ 

**What is a Runnable?**

A Runnable is:

> Anything that takes an input and produces an output

Thatâ€™s it. No special meaning.

**Formally:**

`Runnable[I, O]`

**Examples:**

- `Runnable[str, list[Document]]` â†’ retriever
- `Runnable[dict, str]` â†’ prompt
- `Runnable[str, str]` â†’ LLM
- `Runnable[str, Any]` â†’ parser

---

## 4ï¸âƒ£ Why Runnables Solve â€œToo Many Componentsâ€

Instead of this ğŸ‘:

* `LLMChain`
* `RetrievalQA`
* `StuffDocumentsChain`
* `MapReduceChain`
* `SequentialChain`

They now have this ğŸ‘:

* Runnable + composition

Everything becomes:

- simple
- composable
- debuggable
- reusable
- No special classes per use case

---

## 5ï¸âƒ£ How Composition Works (This is the Key)

Runnables can be chained using operators:

**Pipe (`|`) operator**

```python
prompt | llm | parser
```

Means:

```python
output = parser(llm(prompt(input)))
```

This replaces `LLMChain` entirely.

---

## 6ï¸âƒ£ RAG Expressed with Runnables (Matches Your Diagram)

Your manual RAG flow:

```
query
 â†’ retriever
 â†’ format docs
 â†’ prompt
 â†’ LLM
 â†’ answer
```

Runnable version:

```python
chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | parser
)
```

No special â€œRAG chainâ€ class. Just composition.

---

## 7ï¸âƒ£ Why This is Better Than Old Chains

**Old way (rigid):**

- Behavior hidden inside classes
- Hard to inject logic
- Hard to debug intermediate outputs

**Runnable way (transparent):**

- Every step visible
- Every step testable
- Swap any part easily
- Works for all AI workflows

---

## 8ï¸âƒ£ Intent Detection Example (from Your Diagram)

**Old approach:**

- Custom intent chain
- Custom router chain
- Custom glue

**Runnable approach:**

```python
intent_chain = intent_prompt | llm | parser

router = RunnableBranch(
    (is_search, search_chain),
    (is_chat, chat_chain),
    default_chain
)
```

Intent routing is now just another runnable.

---

## 9ï¸âƒ£ Mental Model Shift (Very Important)

**Before:**

> â€œWhich LangChain class should I use?â€

**Now:**

> â€œHow do I compose Runnables?â€

LangChain stopped being a framework and became a composition layer.